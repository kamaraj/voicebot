{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üéØ RAG Interview Questions - Accenture\n",
                "\n",
                "**Retrieval-Augmented Generation - Complete Guide**  \n",
                "**Total Questions:** 60\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìì Contents\n",
                "\n",
                "| Section | Questions |\n",
                "|---------|----------|\n",
                "| üî∑ **RAG Fundamentals** | Q1-Q10 |\n",
                "| üî∑ **Chunking Strategies** | Q11-Q18 |\n",
                "| üî∑ **Embeddings & Vector Search** | Q19-Q28 |\n",
                "| üî∑ **Retrieval Optimization** | Q29-Q38 |\n",
                "| üî∑ **Advanced RAG Techniques** | Q39-Q48 |\n",
                "| üî∑ **Evaluation & Production** | Q49-Q55 |\n",
                "| üî∑ **Enterprise & System Design** | Q56-Q60 |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üî∑ RAG Fundamentals (Q1-Q10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. What is RAG (Retrieval-Augmented Generation)?\n",
                "\n",
                "RAG combines LLMs with external knowledge retrieval. Instead of relying solely on trained knowledge, it retrieves relevant documents and includes them as context for generation. This reduces hallucinations and enables up-to-date, domain-specific responses."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. What are the main components of a RAG pipeline?\n",
                "\n",
                "1. **Document Ingestion**: Load and preprocess documents\n",
                "2. **Chunking**: Split documents into manageable pieces\n",
                "3. **Embedding**: Convert chunks to vector representations\n",
                "4. **Vector Store**: Store and index embeddings\n",
                "5. **Retrieval**: Find relevant chunks for a query\n",
                "6. **Generation**: LLM generates response using retrieved context"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. When should you use RAG vs Fine-tuning?\n",
                "\n",
                "| Use Case | RAG | Fine-tuning |\n",
                "|----------|-----|-------------|\n",
                "| Domain knowledge | ‚úÖ Best | Possible |\n",
                "| Frequently updated data | ‚úÖ | ‚ùå |\n",
                "| Need citations | ‚úÖ | ‚ùå |\n",
                "| Style/behavior change | ‚ùå | ‚úÖ |\n",
                "| Limited compute | ‚úÖ | ‚ùå |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. How does RAG reduce hallucinations?\n",
                "\n",
                "RAG grounds LLM responses in retrieved documents rather than relying on parametric memory. The model generates answers based on provided context, which can be verified. This factual grounding significantly reduces fabricated information."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. What is Naive RAG vs Advanced RAG?\n",
                "\n",
                "**Naive RAG**: Simple retrieve-then-generate pipeline. Issues: retrieval inaccuracy, no optimization.\n",
                "\n",
                "**Advanced RAG**: Adds pre-retrieval (query transformation), post-retrieval (reranking, compression), and iterative refinement. Better accuracy and relevance."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6. What are the limitations of RAG?\n",
                "\n",
                "- Retrieval quality bottleneck\n",
                "- Context window limitations\n",
                "- Latency from retrieval step\n",
                "- Chunking may lose context\n",
                "- Cannot learn new behaviors (only knowledge)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7. What is the typical data flow in RAG?\n",
                "\n",
                "```\n",
                "User Query ‚Üí Query Embedding ‚Üí Vector Search ‚Üí \n",
                "Retrieved Chunks ‚Üí Prompt Construction ‚Üí LLM ‚Üí Response\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 8. What frameworks are used for building RAG systems?\n",
                "\n",
                "| Framework | Strengths |\n",
                "|-----------|----------|\n",
                "| **LangChain** | Comprehensive, agents, chains |\n",
                "| **LlamaIndex** | Data indexing, retrieval focus |\n",
                "| **Haystack** | Production-ready, modular |\n",
                "| **Semantic Kernel** | Microsoft ecosystem |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 9. What is the role of context in RAG?\n",
                "\n",
                "Context is the retrieved information injected into the prompt. Quality context leads to accurate answers. Irrelevant context confuses the LLM. Context management (selection, ordering, compression) is critical for RAG success."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 10. How do you handle the \"Lost in the Middle\" problem?\n",
                "\n",
                "LLMs pay less attention to information in the middle of long contexts. Solutions:\n",
                "- Place important content at beginning/end\n",
                "- Use reranking to prioritize\n",
                "- Limit context length\n",
                "- Use recursive summarization"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üî∑ Chunking Strategies (Q11-Q18)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 11. What is chunking and why is it important?\n",
                "\n",
                "Chunking splits documents into smaller pieces for embedding and retrieval. Important because: embeddings work best on focused content, retrieval returns relevant portions only, and it fits within context limits."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 12. What are the main chunking strategies?\n",
                "\n",
                "| Strategy | Description |\n",
                "|----------|-------------|\n",
                "| **Fixed-size** | Split by token/character count |\n",
                "| **Sentence-based** | Split at sentence boundaries |\n",
                "| **Paragraph-based** | Split at paragraphs |\n",
                "| **Semantic** | Split by meaning/topics |\n",
                "| **Recursive** | Hierarchical splitting |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 13. What is the optimal chunk size?\n",
                "\n",
                "Depends on use case. General guidelines:\n",
                "- **Small (128-256 tokens)**: Precise retrieval, may lose context\n",
                "- **Medium (512-1024 tokens)**: Good balance (most common)\n",
                "- **Large (1024+ tokens)**: More context, less precise\n",
                "\n",
                "Always test on your specific data and queries."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 14. What is overlapping/sliding window chunking?\n",
                "\n",
                "Chunks share some content with neighbors (e.g., 10-20% overlap). Benefits:\n",
                "- Preserves context at boundaries\n",
                "- Reduces information loss\n",
                "- Improves retrieval for boundary-spanning queries"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 15. What is semantic chunking?\n",
                "\n",
                "Splits text based on meaning rather than fixed size. Uses embeddings to detect topic shifts. Keeps semantically related content together. More complex but preserves context better than fixed-size chunking."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 16. What is contextual chunking?\n",
                "\n",
                "Enhances chunks with additional context:\n",
                "- Adds document summary to each chunk\n",
                "- Includes section headers\n",
                "- Appends adjacent chunk summaries\n",
                "\n",
                "Helps retrieval understand chunk in broader context."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 17. What is agentic/structural chunking?\n",
                "\n",
                "Uses document structure (headings, sections) to guide chunking. May use LLM to identify logical boundaries. Best for structured documents like reports, manuals, legal documents."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 18. How do you handle tables and images in chunking?\n",
                "\n",
                "- **Tables**: Keep together, convert to text/markdown, or use specialized table embeddings\n",
                "- **Images**: Extract text (OCR), generate captions, use multimodal embeddings\n",
                "- Store metadata linking to original assets"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üî∑ Embeddings & Vector Search (Q19-Q28)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 19. What are embeddings in RAG?\n",
                "\n",
                "Embeddings are dense vector representations of text that capture semantic meaning. Similar meanings result in similar vectors. They enable semantic search - finding relevant content based on meaning, not just keywords."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 20. What embedding models are commonly used?\n",
                "\n",
                "| Model | Provider | Dimensions |\n",
                "|-------|----------|------------|\n",
                "| text-embedding-3-large | OpenAI | 3072 |\n",
                "| text-embedding-3-small | OpenAI | 1536 |\n",
                "| BGE-large | BAAI | 1024 |\n",
                "| E5-large | Microsoft | 1024 |\n",
                "| Cohere embed-v3 | Cohere | 1024 |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 21. What is a Vector Database?\n",
                "\n",
                "Specialized database for storing and querying high-dimensional vectors. Optimized for similarity search. Examples: Pinecone, Weaviate, Chroma, Qdrant, Milvus, pgvector."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 22. What is Cosine Similarity?\n",
                "\n",
                "Measures angle between vectors (not magnitude). Range: -1 to 1 (1 = identical). Most common similarity metric for embeddings. Formula: cos(Œ∏) = (A¬∑B) / (||A|| √ó ||B||)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 23. What is ANN (Approximate Nearest Neighbor) search?\n",
                "\n",
                "Trades perfect accuracy for speed. Finds \"close enough\" matches much faster than exact search. Essential for large-scale production systems. Algorithms: HNSW, IVF, LSH."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 24. What is HNSW (Hierarchical Navigable Small World)?\n",
                "\n",
                "Popular ANN algorithm using multi-layer graph structure. Fast queries with high recall. Used by Pinecone, Weaviate, pgvector. Good balance of speed and accuracy."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 25. How do you choose a vector database?\n",
                "\n",
                "Consider: scale (millions of vectors?), latency requirements, managed vs self-hosted, hybrid search support, filtering capabilities, cost, existing infrastructure."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 26. What is metadata filtering in vector search?\n",
                "\n",
                "Combines vector similarity with metadata constraints. Example: Find similar documents WHERE date > 2023 AND category = 'finance'. Reduces search space, improves relevance."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 27. How do you optimize embedding quality?\n",
                "\n",
                "- Choose domain-appropriate model\n",
                "- Fine-tune on your data if needed\n",
                "- Benchmark on your queries\n",
                "- Use instruction-tuned models\n",
                "- Consider query vs document embeddings"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 28. What is the embedding asymmetry problem?\n",
                "\n",
                "Queries are short, documents are long - different embedding characteristics. Solutions:\n",
                "- Use asymmetric embedding models\n",
                "- HyDE (generate hypothetical answer)\n",
                "- Query expansion"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üî∑ Retrieval Optimization (Q29-Q38)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 29. What is Hybrid Search?\n",
                "\n",
                "Combines dense (semantic) and sparse (keyword) retrieval:\n",
                "- **Dense**: Vector embeddings, semantic meaning\n",
                "- **Sparse**: BM25/TF-IDF, exact keywords\n",
                "\n",
                "Merged using RRF (Reciprocal Rank Fusion). Better than either alone."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 30. What is Reranking?\n",
                "\n",
                "Two-stage retrieval:\n",
                "1. Fast initial retrieval (top-K candidates)\n",
                "2. Precise reranking with cross-encoder\n",
                "\n",
                "Cross-encoders process query+document together for better relevance scoring. Models: Cohere Rerank, BGE-reranker."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 31. What is Query Expansion?\n",
                "\n",
                "Generate multiple query variations to improve recall:\n",
                "- Synonyms and related terms\n",
                "- LLM-generated alternatives\n",
                "- Multi-query retrieval\n",
                "\n",
                "Catches documents matching different phrasings."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 32. What is HyDE (Hypothetical Document Embeddings)?\n",
                "\n",
                "1. LLM generates hypothetical answer to query\n",
                "2. Embed the hypothetical answer\n",
                "3. Search for similar real documents\n",
                "\n",
                "Bridges query-document embedding gap. Often improves retrieval quality."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 33. What is Contextual Compression?\n",
                "\n",
                "Extract only relevant portions from retrieved documents:\n",
                "- LLM extracts relevant sentences\n",
                "- Summarizes long chunks\n",
                "- Removes irrelevant content\n",
                "\n",
                "Reduces noise, fits more documents in context."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 34. What is Multi-hop Retrieval?\n",
                "\n",
                "For complex queries requiring information from multiple sources:\n",
                "1. Initial retrieval\n",
                "2. Reason about what's missing\n",
                "3. Retrieve additional information\n",
                "4. Repeat until sufficient\n",
                "\n",
                "Example: \"Who is the CEO of the company that made the iPhone?\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 35. What is Parent-Child Retrieval?\n",
                "\n",
                "- Embed small chunks (children) for precise retrieval\n",
                "- Return larger chunks (parents) for context\n",
                "- Best of both: precision + context\n",
                "\n",
                "Also called \"Small-to-Big\" retrieval."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 36. What is Semantic Caching?\n",
                "\n",
                "Cache query embeddings and responses. For semantically similar queries, return cached answers. Benefits:\n",
                "- Reduces LLM API calls\n",
                "- Lower latency (ms vs seconds)\n",
                "- Cost savings"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 37. How do you optimize retrieval latency?\n",
                "\n",
                "- Use efficient ANN indexes (HNSW)\n",
                "- Limit retrieved documents (top-K)\n",
                "- Metadata pre-filtering\n",
                "- Caching\n",
                "- Async retrieval\n",
                "- Index sharding for scale"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 38. What is Corrective RAG (CRAG)?\n",
                "\n",
                "Adds verification step:\n",
                "1. Retrieve documents\n",
                "2. LLM evaluates relevance of each\n",
                "3. Discard irrelevant ones\n",
                "4. Optionally trigger web search if context insufficient\n",
                "\n",
                "Self-correcting retrieval improves accuracy."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üî∑ Advanced RAG Techniques (Q39-Q48)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 39. What is Adaptive RAG?\n",
                "\n",
                "Dynamically adjusts retrieval based on query:\n",
                "- Simple queries: No retrieval needed\n",
                "- Moderate: Single retrieval\n",
                "- Complex: Multi-hop/iterative\n",
                "\n",
                "Classifier routes queries to appropriate strategy."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 40. What is Self-RAG?\n",
                "\n",
                "LLM decides when and what to retrieve:\n",
                "1. Generate initial response\n",
                "2. Self-critique for gaps\n",
                "3. Retrieve if needed\n",
                "4. Refine response\n",
                "\n",
                "More autonomous, reduces unnecessary retrieval."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 41. What is GraphRAG?\n",
                "\n",
                "Combines RAG with knowledge graphs:\n",
                "- Extract entities and relationships\n",
                "- Build graph structure\n",
                "- Retrieve via graph traversal\n",
                "\n",
                "Better for multi-hop reasoning, entity relationships."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 42. What is Multimodal RAG?\n",
                "\n",
                "Extends RAG to multiple modalities:\n",
                "- Text + Images\n",
                "- Text + Tables\n",
                "- Text + Audio/Video\n",
                "\n",
                "Uses multimodal embeddings, image understanding models."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 43. What is Agentic RAG?\n",
                "\n",
                "RAG enhanced with agent capabilities:\n",
                "- Query planning and decomposition\n",
                "- Tool use (calculators, APIs)\n",
                "- Iterative retrieval and reasoning\n",
                "- Self-reflection and correction"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 44. What is Modular RAG?\n",
                "\n",
                "Flexible architecture with interchangeable components:\n",
                "- Swap embedding models\n",
                "- Change retrieval strategies\n",
                "- Add/remove processing steps\n",
                "\n",
                "Enables experimentation and optimization."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 45. How do you handle long documents?\n",
                "\n",
                "- Recursive summarization (tree structure)\n",
                "- Map-reduce: summarize chunks, then combine\n",
                "- Hierarchical indexing\n",
                "- Use long-context models (Gemini 1M tokens)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 46. What is RAG Fusion?\n",
                "\n",
                "1. Generate multiple query variations\n",
                "2. Retrieve for each query\n",
                "3. Fuse results using RRF\n",
                "4. Generate from combined context\n",
                "\n",
                "Improves recall by covering different angles."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 47. How do you handle real-time/streaming data?\n",
                "\n",
                "- Incremental indexing\n",
                "- Time-based partitioning\n",
                "- TTL (time-to-live) for freshness\n",
                "- Hybrid with real-time API calls"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 48. What is the role of prompts in RAG?\n",
                "\n",
                "Critical for RAG success:\n",
                "- System prompt: Define behavior, citation format\n",
                "- Context injection: How to present retrieved docs\n",
                "- Instructions: Use only provided context, cite sources\n",
                "- Output format: JSON, structured responses"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üî∑ Evaluation & Production (Q49-Q55)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 49. What is RAGAS?\n",
                "\n",
                "RAG Assessment framework with metrics:\n",
                "- **Faithfulness**: Is answer grounded in context?\n",
                "- **Answer Relevancy**: Does answer address query?\n",
                "- **Context Precision**: Is retrieved context relevant?\n",
                "- **Context Recall**: Is all needed info retrieved?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 50. What retrieval metrics should you track?\n",
                "\n",
                "| Metric | Description |\n",
                "|--------|-------------|\n",
                "| Precision@K | % of retrieved docs that are relevant |\n",
                "| Recall@K | % of relevant docs retrieved |\n",
                "| MRR | Mean Reciprocal Rank |\n",
                "| nDCG | Normalized Discounted Cumulative Gain |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 51. What generation metrics should you track?\n",
                "\n",
                "- **Faithfulness/Groundedness**: Factual accuracy\n",
                "- **Answer Relevance**: Addresses the question\n",
                "- **Completeness**: Covers all aspects\n",
                "- **Coherence**: Well-structured response\n",
                "- **Hallucination rate**: Fabricated content"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 52. How do you evaluate RAG in production?\n",
                "\n",
                "- User feedback (thumbs up/down)\n",
                "- LLM-as-judge for automated eval\n",
                "- A/B testing for changes\n",
                "- Human evaluation samples\n",
                "- Logging and monitoring dashboards"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 53. What are common RAG failure modes?\n",
                "\n",
                "1. **Retrieval failure**: Wrong documents retrieved\n",
                "2. **Context overflow**: Too much irrelevant info\n",
                "3. **Lost in middle**: Important info ignored\n",
                "4. **Hallucination**: Ignores context, makes up info\n",
                "5. **Incomplete**: Missing key information"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 54. How do you debug RAG issues?\n",
                "\n",
                "1. Log retrieved chunks for each query\n",
                "2. Check retrieval relevance scores\n",
                "3. Inspect prompt construction\n",
                "4. Test components in isolation\n",
                "5. Create evaluation test sets"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 55. What production monitoring is needed?\n",
                "\n",
                "- Latency (retrieval, generation, E2E)\n",
                "- Token usage and costs\n",
                "- Error rates\n",
                "- User satisfaction metrics\n",
                "- Index freshness"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üî∑ Enterprise & System Design (Q56-Q60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 56. Design a RAG system for enterprise Q&A.\n",
                "\n",
                "```\n",
                "1. Data Ingestion: Connectors for various sources\n",
                "2. Processing: Chunking, cleaning, metadata extraction\n",
                "3. Embedding: Batch processing, incremental updates\n",
                "4. Vector Store: Scalable, filtered search\n",
                "5. Retrieval: Hybrid search + reranking\n",
                "6. Generation: Prompt engineering, guardrails\n",
                "7. Frontend: Chat interface, citation display\n",
                "8. Monitoring: Metrics, logging, feedback\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 57. How do you handle security in enterprise RAG?\n",
                "\n",
                "- **Access control**: User-based document permissions\n",
                "- **Data privacy**: PII detection and masking\n",
                "- **Audit logging**: Track all queries and responses\n",
                "- **Prompt injection**: Input validation\n",
                "- **Data residency**: On-premise or private cloud"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 58. How do you scale RAG for millions of documents?\n",
                "\n",
                "- Distributed vector database (sharding)\n",
                "- Efficient indexing (HNSW, IVF)\n",
                "- Metadata pre-filtering\n",
                "- Caching layer\n",
                "- Async processing\n",
                "- Load balancing"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 59. How do you keep the knowledge base fresh?\n",
                "\n",
                "- Scheduled re-indexing\n",
                "- Incremental updates (CDC)\n",
                "- Document versioning\n",
                "- TTL-based expiration\n",
                "- Monitoring for staleness"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 60. What is Accenture's approach to enterprise RAG?\n",
                "\n",
                "Accenture emphasizes:\n",
                "- **Digital Core**: Strong data foundation\n",
                "- **Responsible AI**: Governance, compliance\n",
                "- **Human+AI**: Augmentation, not replacement\n",
                "- **Value-led**: ROI-focused use cases\n",
                "- **Scalability**: Enterprise-grade architecture\n",
                "\n",
                "---\n",
                "\n",
                "### üçÄ Good luck with your Accenture interview! üí™"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}