<!DOCTYPE html>
<html>

<head>
    <title>VoiceBot - Audio Recording Test</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 50px auto;
            padding: 20px;
            background: #f5f5f5;
        }

        .container {
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }

        h1 {
            color: #667eea;
            margin-bottom: 20px;
        }

        .voice-btn {
            width: 100px;
            height: 100px;
            border-radius: 50%;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            border: none;
            color: white;
            font-size: 40px;
            cursor: pointer;
            margin: 20px auto;
            display: block;
            transition: all 0.3s;
        }

        .voice-btn:hover {
            transform: scale(1.1);
        }

        .voice-btn.recording {
            background: linear-gradient(135deg, #ef4444 0%, #dc2626 100%);
            animation: pulse 1.5s infinite;
        }

        @keyframes pulse {

            0%,
            100% {
                box-shadow: 0 0 0 0 rgba(239, 68, 68, 0.7);
            }

            50% {
                box-shadow: 0 0 0 30px rgba(239, 68, 68, 0);
            }
        }

        .status {
            text-align: center;
            font-size: 18px;
            color: #666;
            margin: 20px 0;
            min-height: 30px;
            font-weight: 500;
        }

        .audio-level {
            width: 100%;
            height: 60px;
            background: #f0f0f0;
            border-radius: 8px;
            margin: 20px 0;
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 4px;
            padding: 10px;
        }

        .level-bar {
            width: 8px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            border-radius: 4px;
            transition: height 0.1s;
        }

        .transcript {
            background: linear-gradient(135deg, rgba(102, 126, 234, 0.1) 0%, rgba(118, 75, 162, 0.1) 100%);
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            min-height: 80px;
            border: 2px solid #667eea;
            display: none;
        }

        .transcript-label {
            font-weight: bold;
            color: #667eea;
            margin-bottom: 10px;
            font-size: 14px;
        }

        .transcript-text {
            color: #333;
            font-size: 16px;
            line-height: 1.6;
        }

        .info {
            background: #e8f5e9;
            padding: 15px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 4px solid #4caf50;
        }

        .timer {
            text-align: center;
            font-size: 24px;
            font-weight: bold;
            color: #667eea;
            margin: 10px 0;
        }
    </style>
</head>

<body>
    <div class="container">
        <h1>üé§ VoiceBot - Audio Recording (Works in ALL Browsers)</h1>

        <div class="info">
            <strong>‚úÖ This version:</strong>
            <ul style="margin: 10px 0; padding-left: 20px;">
                <li>Records REAL audio from your microphone</li>
                <li>Shows visual feedback (audio levels)</li>
                <li>10 seconds max recording</li>
                <li>3 seconds silence auto-submit</li>
                <li>Works in ALL browsers (Chrome, Firefox, Edge, Safari)</li>
            </ul>
        </div>

        <div class="timer" id="timer">0s / 10s</div>

        <button class="voice-btn" id="voiceBtn" onclick="toggleRecording()">üé§</button>

        <div class="status" id="status">Click microphone to start recording</div>

        <div class="audio-level" id="audioLevel">
            <div class="level-bar" style="height: 10px;"></div>
            <div class="level-bar" style="height: 10px;"></div>
            <div class="level-bar" style="height: 10px;"></div>
            <div class="level-bar" style="height: 10px;"></div>
            <div class="level-bar" style="height: 10px;"></div>
            <div class="level-bar" style="height: 10px;"></div>
            <div class="level-bar" style="height: 10px;"></div>
            <div class="level-bar" style="height: 10px;"></div>
        </div>

        <div class="transcript" id="transcriptBox">
            <div class="transcript-label">üìù Recorded Audio:</div>
            <div class="transcript-text" id="transcript"></div>
        </div>
    </div>

    <script>
        let mediaRecorder = null;
        let audioChunks = [];
        let isRecording = false;
        let audioContext = null;
        let analyser = null;
        let silenceTimer = null;
        let maxTimer = null;
        let timerInterval = null;
        let lastSoundTime = Date.now();
        let recordingStartTime = null;

        async function toggleRecording() {
            if (!isRecording) {
                await startRecording();
            } else {
                stopRecording();
            }
        }

        async function startRecording() {
            try {
                // Request microphone with specific constraints
                const constraints = {
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true,
                        channelCount: 1,
                        sampleRate: 48000
                    }
                };

                console.log('üé§ Requesting microphone access...');
                const stream = await navigator.mediaDevices.getUserMedia(constraints);

                console.log('‚úÖ Microphone access granted');
                console.log('üìã Audio tracks:', stream.getAudioTracks().map(t => t.label));

                // Setup audio context for visualization
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                console.log('üîä Audio context created, sample rate:', audioContext.sampleRate);

                // Resume audio context if suspended
                if (audioContext.state === 'suspended') {
                    await audioContext.resume();
                    console.log('‚ñ∂Ô∏è Audio context resumed');
                }

                const source = audioContext.createMediaStreamSource(stream);
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 2048; // Increased for better sensitivity
                analyser.smoothingTimeConstant = 0.8;
                source.connect(analyser);

                console.log('üìä Analyzer connected, FFT size:', analyser.fftSize);

                // Setup media recorder
                mediaRecorder = new MediaRecorder(stream);
                audioChunks = [];

                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                    }
                };

                mediaRecorder.onstop = () => {
                    processAudio();
                };

                // Start recording
                mediaRecorder.start();
                isRecording = true;
                recordingStartTime = Date.now();
                lastSoundTime = Date.now();

                // UI updates
                document.getElementById('voiceBtn').classList.add('recording');
                document.getElementById('voiceBtn').textContent = '‚èπÔ∏è';
                document.getElementById('status').textContent = 'üé§ Recording... speak now! (Speak loudly)';

                // Start visualizer
                visualizeAudio();

                // Timer display
                timerInterval = setInterval(() => {
                    const elapsed = Math.floor((Date.now() - recordingStartTime) / 1000);
                    document.getElementById('timer').textContent = `${elapsed}s / 10s`;
                }, 100);

                // Max 10 seconds
                maxTimer = setTimeout(() => {
                    document.getElementById('status').textContent = '‚è±Ô∏è 10 seconds reached!';
                    stopRecording();
                }, 10000);

                // Silence detection
                detectSilence();

            } catch (error) {
                console.error('‚ùå Microphone error:', error);
                let errorMsg = '‚ùå Microphone Error:\n\n';

                if (error.name === 'NotAllowedError') {
                    errorMsg += 'Permission denied!\n\n';
                    errorMsg += '1. Click the üé§ icon in the address bar\n';
                    errorMsg += '2. Select "Allow"\n';
                    errorMsg += '3. Refresh the page';
                } else if (error.name === 'NotFoundError') {
                    errorMsg += 'No microphone found!\n\n';
                    errorMsg += '1. Connect a microphone\n';
                    errorMsg += '2. Check device settings';
                } else {
                    errorMsg += error.message;
                }

                document.getElementById('status').textContent = errorMsg;
                alert(errorMsg);
            }
        }

        function visualizeAudio() {
            if (!isRecording || !analyser) return;

            const dataArray = new Uint8Array(analyser.frequencyBinCount);
            analyser.getByteFrequencyData(dataArray);

            const bars = document.querySelectorAll('.level-bar');
            const average = dataArray.reduce((a, b) => a + b) / dataArray.length;

            // Debug: Show audio level occasionally (not every frame)
            if (Math.random() < 0.02) { // Only 2% of the time
                console.log('üìä Audio level:', Math.round(average));
            }

            // Update each bar based on frequency data
            bars.forEach((bar, index) => {
                const value = dataArray[index * 4] || 0;
                const height = Math.max(10, (value / 255) * 50);
                bar.style.height = height + 'px';
            });

            // Detect sound vs silence (threshold: 10 for better sensitivity)
            if (average > 10) {
                lastSoundTime = Date.now();
                if (Math.random() < 0.05) { // Only log occasionally
                    console.log('üé§ Sound detected, level:', Math.round(average));
                }
            }

            requestAnimationFrame(visualizeAudio);
        }

        function detectSilence() {
            silenceTimer = setInterval(() => {
                if (!isRecording) return;

                const silenceDuration = (Date.now() - lastSoundTime) / 1000;
                const elapsed = Math.floor((Date.now() - recordingStartTime) / 1000);

                if (silenceDuration >= 3) {
                    document.getElementById('status').textContent = '‚úÖ 3 seconds of silence - processing!';
                    stopRecording();
                } else if (silenceDuration >= 1) {
                    const remaining = Math.ceil(3 - silenceDuration);
                    document.getElementById('status').textContent = `ü§´ Silence detected - auto-submit in ${remaining}s... (${elapsed}s / 10s)`;
                } else {
                    document.getElementById('status').textContent = `üé§ Recording... (${elapsed}s / 10s max)`;
                }
            }, 100);
        }

        function stopRecording() {
            if (!isRecording) return;

            isRecording = false;
            clearTimeout(maxTimer);
            clearInterval(silenceTimer);
            clearInterval(timerInterval);

            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
                mediaRecorder.stream.getTracks().forEach(track => track.stop());
            }

            // UI updates
            document.getElementById('voiceBtn').classList.remove('recording');
            document.getElementById('voiceBtn').textContent = 'üé§';

            // Reset bars
            document.querySelectorAll('.level-bar').forEach(bar => {
                bar.style.height = '10px';
            });
        }

        function processAudio() {
            document.getElementById('status').textContent = 'üîÑ Processing audio...';

            const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
            const audioSize = (audioBlob.size / 1024).toFixed(2);

            // Show transcript box
            document.getElementById('transcriptBox').style.display = 'block';
            document.getElementById('transcript').textContent =
                `‚úÖ Recorded ${audioSize} KB of audio\n\n` +
                `‚ö†Ô∏è Note: This is a DEMO version.\n` +
                `The audio was successfully captured!\n\n` +
                `To get real transcription:\n` +
                `1. Install Whisper: pip install openai-whisper\n` +
                `2. Add backend endpoint to process audio\n` +
                `3. Send this audioBlob to the backend\n\n` +
                `For now, type your message instead or use the unified chat at /chat`;

            document.getElementById('status').textContent = '‚úÖ Audio recorded! (See below)';
            document.getElementById('timer').textContent = '0s / 10s';

            // In production, send audioBlob to backend:
            // sendAudioToBackend(audioBlob);
        }

        async function sendAudioToBackend(audioBlob) {
            const formData = new FormData();
            formData.append('audio', audioBlob, 'recording.wav');

            try {
                const response = await fetch('http://localhost:9011/api/v1/voice/transcribe', {
                    method: 'POST',
                    body: formData
                });

                const data = await response.json();
                document.getElementById('transcript').textContent = data.transcript;

                // Send transcript to LLM
                const llmResponse = await fetch('http://localhost:9011/api/v1/conversation', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ message: data.transcript })
                });

                const llmData = await llmResponse.json();
                document.getElementById('transcript').textContent +=
                    `\n\nü§ñ AI: ${llmData.response}`;

            } catch (error) {
                console.error('Backend error:', error);
            }
        }
    </script>
</body>

</html>